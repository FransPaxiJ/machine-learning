{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb5c938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.203 üöÄ Python-3.10.18 torch-2.8.0 CPU (Apple M4 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=detector_comportamiento_sospechoso2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/opt/homebrew/runs/detect/detector_comportamiento_sospechoso2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  2    180864  ultralytics.nn.modules.block.A2C2f           [128, 128, 2, True, 4]        \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1     86912  ultralytics.nn.modules.block.A2C2f           [384, 128, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     24000  ultralytics.nn.modules.block.A2C2f           [256, 64, 1, False, -1]       \n",
      " 15                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     74624  ultralytics.nn.modules.block.A2C2f           [192, 128, 1, False, -1]      \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 21        [14, 17, 20]  1    431452  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "YOLOv12n summary: 272 layers, 2,568,828 parameters, 2,568,812 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 640/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 75.8¬±50.8 MB/s, size: 19.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/train/labels.cache... 2721 images, 6 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2721/2721 7.9Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/train/images/106473821-1585931052853woman-makes-purchases-in-the-store-novelty-consultation-woman-adult-fashion-consumer-indoor-human_t20_2j43do_jpg.rf.0b1ba8b7a2ed8c803456ae287ad453d4.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/train/images/106473821-1585931052853woman-makes-purchases-in-the-store-novelty-consultation-woman-adult-fashion-consumer-indoor-human_t20_2j43do_jpg.rf.92ea7595e4e5cede13e0a0420d1d85c6.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/train/images/106473821-1585931052853woman-makes-purchases-in-the-store-novelty-consultation-woman-adult-fashion-consumer-indoor-human_t20_2j43do_jpg.rf.b8ad50e3d18afb28d2710251b581baa4.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/train/images/3205-23-Robbery-108-Pct-11-24-23-1_webp.rf.17542042955ab82c44644b64b301971c.jpg: 9 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/train/images/3205-23-Robbery-108-Pct-11-24-23-1_webp.rf.86b429e2a58f8068a8dc2e88dbc34b53.jpg: 9 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/train/images/3205-23-Robbery-108-Pct-11-24-23-1_webp.rf.f7b0fb16244ff1d83b767c8893b1e929.jpg: 9 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/train/images/hq720-1-_jpg.rf.00a59388df6bc1e61a447857e0b60e5d.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/train/images/hq720-1-_jpg.rf.70cf65ad4f077c25f1fdf79bfa9425af.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/train/images/hq720-1-_jpg.rf.b84dcd45c821b3425139706f9746066c.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/train/images/person-detection-ai-camera_jpg.rf.43a0f61f5943c15095d5f95fc91335bd.jpg: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/train/images/person-detection-ai-camera_jpg.rf.cfd3ecb960c5ea9f26c5d4e59a02bb7b.jpg: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/train/images/person-detection-ai-camera_jpg.rf.f69ed2b40a6f40a2d822d2235c9d27a1.jpg: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 122.9¬±40.8 MB/s, size: 20.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/valid/labels.cache... 672 images, 4 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 672/672 3.3Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/valid/images/images-11-_jpg.rf.c775161185f1be74c27b8fad17bb5e2e.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/franspaxi/machine-learning/unidad_2/Suspicious Behavior/valid/images/images-16-_jpg.rf.213088027b12378cdce1c6afbfd45257.jpg: 1 duplicate labels removed\n",
      "Plotting labels to /opt/homebrew/runs/detect/detector_comportamiento_sospechoso2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/opt/homebrew/runs/detect/detector_comportamiento_sospechoso2\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/1         0G      1.374      2.513       1.57          3        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 341/341 0.5it/s 12:06<1.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 42/42 0.3it/s 2:022.9ss\n",
      "                   all        672       2001      0.476      0.373      0.353      0.182\n",
      "\n",
      "1 epochs completed in 0.236 hours.\n",
      "Optimizer stripped from /opt/homebrew/runs/detect/detector_comportamiento_sospechoso2/weights/last.pt, 5.5MB\n",
      "Optimizer stripped from /opt/homebrew/runs/detect/detector_comportamiento_sospechoso2/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating /opt/homebrew/runs/detect/detector_comportamiento_sospechoso2/weights/best.pt...\n",
      "Ultralytics 8.3.203 üöÄ Python-3.10.18 torch-2.8.0 CPU (Apple M4 Pro)\n",
      "YOLOv12n summary (fused): 159 layers, 2,557,508 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 42/42 0.4it/s 1:542.7sss\n",
      "                   all        672       2001      0.474      0.373      0.353      0.182\n",
      "        Accion anormal        400        441      0.341      0.619      0.453      0.242\n",
      "                  Arma        366        423       0.46      0.139      0.175     0.0835\n",
      "              Cubierto        373        467      0.551      0.407      0.408      0.208\n",
      "        Persona normal        301        670      0.545      0.325      0.374      0.194\n",
      "Speed: 0.4ms preprocess, 163.7ms inference, 0.0ms loss, 3.2ms postprocess per image\n",
      "Results saved to \u001b[1m/opt/homebrew/runs/detect/detector_comportamiento_sospechoso2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    # Cargar modelo base YOLOv11 (yolo12n.pt no existe)\n",
    "    model = YOLO(\"yolo12n.pt\")  # Se descarga autom√°ticamente\n",
    "    \n",
    "    # Verificar que el archivo data.yaml existe\n",
    "    data_path = \"data.yaml\"  # Archivo en la misma carpeta\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Error: No se encuentra {data_path}\")\n",
    "        print(f\"Directorio actual: {os.getcwd()}\")\n",
    "        print(f\"Archivos en el directorio: {os.listdir('.')}\")\n",
    "        return\n",
    "    \n",
    "    # Entrenar con tu dataset\n",
    "    model.train(\n",
    "        data=data_path,  # Ruta corregida\n",
    "        epochs=1,  # Aument√© las √©pocas\n",
    "        batch=8,\n",
    "        imgsz=640,\n",
    "        name=\"detector_comportamiento_sospechoso\"  # Nombre m√°s apropiado\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664c7fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/franspaxi/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Modelo cargado: <class 'ultralytics.models.yolo.model.YOLO'> True\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "# intenta cargar el checkpoint (aseg√∫rate que yolo12n.pt est√© en el cwd o pon ruta completa)\n",
    "m = YOLO(\"yolo12n.pt\")\n",
    "print(\"Modelo cargado:\", type(m), m.model is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff7776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé• Presiona 'q' para salir\n",
      "\n",
      "0: 384x640 5 Accion anormals, 1 Arma, 1 Cubierto, 68.9ms\n",
      "Speed: 5.2ms preprocess, 68.9ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 59.9ms\n",
      "Speed: 1.1ms preprocess, 59.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 61.7ms\n",
      "Speed: 1.2ms preprocess, 61.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 59.0ms\n",
      "Speed: 1.2ms preprocess, 59.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 57.6ms\n",
      "Speed: 1.4ms preprocess, 57.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 59.3ms\n",
      "Speed: 1.3ms preprocess, 59.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 60.2ms\n",
      "Speed: 1.0ms preprocess, 60.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 56.0ms\n",
      "Speed: 1.2ms preprocess, 56.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 55.8ms\n",
      "Speed: 1.2ms preprocess, 55.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Accion anormals, 2 Armas, 1 Cubierto, 56.5ms\n",
      "Speed: 1.5ms preprocess, 56.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Cubierto, 55.9ms\n",
      "Speed: 1.3ms preprocess, 55.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 58.3ms\n",
      "Speed: 1.7ms preprocess, 58.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Accion anormals, 1 Arma, 1 Cubierto, 55.6ms\n",
      "Speed: 1.3ms preprocess, 55.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 52.7ms\n",
      "Speed: 1.1ms preprocess, 52.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 57.6ms\n",
      "Speed: 1.0ms preprocess, 57.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 56.9ms\n",
      "Speed: 1.6ms preprocess, 56.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 57.3ms\n",
      "Speed: 1.5ms preprocess, 57.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 61.9ms\n",
      "Speed: 1.1ms preprocess, 61.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 Accion anormals, 1 Cubierto, 54.9ms\n",
      "Speed: 1.5ms preprocess, 54.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 Accion anormals, 1 Arma, 1 Cubierto, 59.1ms\n",
      "Speed: 1.1ms preprocess, 59.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 56.6ms\n",
      "Speed: 1.0ms preprocess, 56.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 58.4ms\n",
      "Speed: 1.1ms preprocess, 58.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 2 Armas, 1 Cubierto, 53.5ms\n",
      "Speed: 1.2ms preprocess, 53.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 57.0ms\n",
      "Speed: 1.5ms preprocess, 57.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 Accion anormals, 2 Armas, 1 Cubierto, 57.0ms\n",
      "Speed: 1.4ms preprocess, 57.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 2 Armas, 1 Cubierto, 56.3ms\n",
      "Speed: 1.2ms preprocess, 56.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 2 Armas, 1 Cubierto, 60.0ms\n",
      "Speed: 1.2ms preprocess, 60.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 55.3ms\n",
      "Speed: 1.1ms preprocess, 55.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Accion anormals, 1 Cubierto, 56.8ms\n",
      "Speed: 1.4ms preprocess, 56.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Cubierto, 59.5ms\n",
      "Speed: 1.3ms preprocess, 59.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Cubierto, 57.3ms\n",
      "Speed: 1.4ms preprocess, 57.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Cubierto, 55.8ms\n",
      "Speed: 1.2ms preprocess, 55.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Cubierto, 57.3ms\n",
      "Speed: 1.1ms preprocess, 57.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Cubierto, 57.0ms\n",
      "Speed: 1.4ms preprocess, 57.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 58.5ms\n",
      "Speed: 1.2ms preprocess, 58.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 Accion anormals, 1 Cubierto, 57.8ms\n",
      "Speed: 1.1ms preprocess, 57.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 Accion anormals, 1 Cubierto, 58.2ms\n",
      "Speed: 1.0ms preprocess, 58.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 Accion anormals, 1 Cubierto, 58.7ms\n",
      "Speed: 1.1ms preprocess, 58.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Cubierto, 59.3ms\n",
      "Speed: 1.4ms preprocess, 59.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Cubierto, 63.6ms\n",
      "Speed: 1.1ms preprocess, 63.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Cubierto, 58.4ms\n",
      "Speed: 1.8ms preprocess, 58.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 60.6ms\n",
      "Speed: 1.1ms preprocess, 60.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 Accion anormals, 1 Arma, 1 Cubierto, 63.0ms\n",
      "Speed: 1.2ms preprocess, 63.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 64.1ms\n",
      "Speed: 1.4ms preprocess, 64.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 63.6ms\n",
      "Speed: 1.3ms preprocess, 63.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 65.4ms\n",
      "Speed: 1.4ms preprocess, 65.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Cubierto, 56.6ms\n",
      "Speed: 1.2ms preprocess, 56.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Cubierto, 55.0ms\n",
      "Speed: 1.5ms preprocess, 55.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 Accion anormals, 1 Cubierto, 56.5ms\n",
      "Speed: 1.3ms preprocess, 56.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Cubierto, 56.8ms\n",
      "Speed: 1.9ms preprocess, 56.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Cubierto, 59.8ms\n",
      "Speed: 1.5ms preprocess, 59.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Cubierto, 57.5ms\n",
      "Speed: 1.2ms preprocess, 57.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Cubierto, 58.8ms\n",
      "Speed: 1.5ms preprocess, 58.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Cubierto, 57.1ms\n",
      "Speed: 1.0ms preprocess, 57.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 Accion anormals, 1 Cubierto, 56.1ms\n",
      "Speed: 1.2ms preprocess, 56.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 Accion anormals, 1 Arma, 1 Cubierto, 59.2ms\n",
      "Speed: 1.3ms preprocess, 59.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 55.7ms\n",
      "Speed: 1.6ms preprocess, 55.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 Accion anormals, 1 Arma, 1 Cubierto, 55.0ms\n",
      "Speed: 1.2ms preprocess, 55.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 Accion anormals, 54.5ms\n",
      "Speed: 1.5ms preprocess, 54.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 Accion anormals, 1 Arma, 56.1ms\n",
      "Speed: 1.2ms preprocess, 56.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 Accion anormals, 1 Arma, 1 Cubierto, 56.1ms\n",
      "Speed: 1.2ms preprocess, 56.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 Accion anormals, 1 Arma, 57.0ms\n",
      "Speed: 1.2ms preprocess, 57.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 57.9ms\n",
      "Speed: 1.2ms preprocess, 57.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 59.8ms\n",
      "Speed: 1.0ms preprocess, 59.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 59.5ms\n",
      "Speed: 1.4ms preprocess, 59.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 62.3ms\n",
      "Speed: 1.1ms preprocess, 62.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 58.8ms\n",
      "Speed: 1.5ms preprocess, 58.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 Accion anormals, 1 Arma, 1 Cubierto, 58.3ms\n",
      "Speed: 1.3ms preprocess, 58.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 57.1ms\n",
      "Speed: 1.1ms preprocess, 57.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 59.6ms\n",
      "Speed: 1.4ms preprocess, 59.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 58.8ms\n",
      "Speed: 1.2ms preprocess, 58.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 60.5ms\n",
      "Speed: 1.4ms preprocess, 60.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 62.0ms\n",
      "Speed: 1.7ms preprocess, 62.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 56.4ms\n",
      "Speed: 1.0ms preprocess, 56.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 58.7ms\n",
      "Speed: 1.2ms preprocess, 58.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 58.9ms\n",
      "Speed: 1.2ms preprocess, 58.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 58.1ms\n",
      "Speed: 1.2ms preprocess, 58.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 62.7ms\n",
      "Speed: 1.2ms preprocess, 62.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 55.6ms\n",
      "Speed: 1.0ms preprocess, 55.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Accion anormals, 1 Arma, 1 Cubierto, 57.7ms\n",
      "Speed: 1.0ms preprocess, 57.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Accion anormals, 1 Arma, 1 Cubierto, 58.8ms\n",
      "Speed: 1.2ms preprocess, 58.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 60.7ms\n",
      "Speed: 1.1ms preprocess, 60.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 57.7ms\n",
      "Speed: 1.1ms preprocess, 57.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 57.6ms\n",
      "Speed: 1.0ms preprocess, 57.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 57.4ms\n",
      "Speed: 1.7ms preprocess, 57.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 57.8ms\n",
      "Speed: 1.3ms preprocess, 57.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Accion anormals, 1 Arma, 1 Cubierto, 57.6ms\n",
      "Speed: 1.5ms preprocess, 57.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 57.1ms\n",
      "Speed: 1.3ms preprocess, 57.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 62.4ms\n",
      "Speed: 1.2ms preprocess, 62.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 64.3ms\n",
      "Speed: 1.2ms preprocess, 64.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 55.0ms\n",
      "Speed: 1.2ms preprocess, 55.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Accion anormals, 1 Arma, 1 Cubierto, 55.7ms\n",
      "Speed: 1.7ms preprocess, 55.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ C√°mara cerrada\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Ejecutar la funci√≥n\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mprobar_modelo_con_camara\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m, in \u001b[0;36mprobar_modelo_con_camara\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Hacer predicci√≥n en tiempo real\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Dibujar las detecciones en el frame\u001b[39;00m\n\u001b[1;32m     29\u001b[0m annotated_frame \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/ultralytics/engine/model.py:187\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    160\u001b[0m     source: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m Path \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    161\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    163\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    164\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/ultralytics/engine/model.py:557\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/ultralytics/engine/predictor.py:229\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/torch/utils/_contextlib.py:38\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 38\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/ultralytics/engine/predictor.py:336\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 336\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/ultralytics/engine/predictor.py:184\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    180\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    183\u001b[0m )\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/ultralytics/nn/autobackend.py:637\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 637\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/ultralytics/nn/tasks.py:139\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/ultralytics/nn/tasks.py:157\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/ultralytics/nn/tasks.py:180\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 180\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    181\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/ultralytics/nn/modules/head.py:122\u001b[0m, in \u001b[0;36mDetect.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_end2end(x)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnl):\n\u001b[0;32m--> 122\u001b[0m     x[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3[i](x[i])), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:  \u001b[38;5;66;03m# Training path\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/conv.py:548\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/conv.py:543\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    533\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    534\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloque√≥ al ejecutar c√≥digo en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el c√≥digo de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqu√≠</a> para obtener m√°s informaci√≥n. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener m√°s detalles."
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "def probar_modelo_con_camara():\n",
    "    # Cargar tu modelo entrenado\n",
    "    modelo_path = \"best.pt\"\n",
    "    model = YOLO(modelo_path)\n",
    "\n",
    "    # Inicializar la c√°mara (0 = c√°mara predeterminada de la c√°mara)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"‚ùå Error: No se pudo abrir la c√°mara\")\n",
    "        return\n",
    "    \n",
    "    print(\"üé• Presiona 'q' para salir\")\n",
    "    \n",
    "    while True:\n",
    "        # Capturar frame de la c√°mara\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"‚ùå Error al capturar frame\")\n",
    "            break\n",
    "        \n",
    "        # Hacer predicci√≥n en tiempo real\n",
    "        results = model(frame)\n",
    "        \n",
    "        # Dibujar las detecciones en el frame\n",
    "        annotated_frame = results[0].plot()\n",
    "        \n",
    "        # Mostrar el frame con detecciones\n",
    "        cv2.imshow('Detector de Comportamiento Sospechoso', annotated_frame)\n",
    "        \n",
    "        # Salir con 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Limpiar\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"‚úÖ C√°mara cerrada\")\n",
    "\n",
    "# Ejecutar la funci√≥n\n",
    "probar_modelo_con_camara()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c2bebf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Clases del modelo:\n",
      "Total de clases: 9\n",
      "\n",
      "Lista de clases:\n",
      "  0: bank_info\n",
      "  1: communication\n",
      "  2: customer_info\n",
      "  3: invoice_detail\n",
      "  4: logo\n",
      "  5: paragraph\n",
      "  6: table\n",
      "  7: total_price\n",
      "  8: vendor_info\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Cargar el modelo\n",
    "model = YOLO(\"best.pt\")\n",
    "\n",
    "# Ver todas las clases\n",
    "print(\"üìä Clases del modelo:\")\n",
    "print(f\"Total de clases: {len(model.names)}\")\n",
    "print(\"\\nLista de clases:\")\n",
    "for idx, nombre in model.names.items():\n",
    "    print(f\"  {idx}: {nombre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "808f0355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Cargando modelo...\n",
      "üìÑ Se encontraron 3 archivos PDF\n",
      "\n",
      "üîç Procesando: 20427799973-03-b090-00053521_uzw3qr4qjh.pdf\n",
      "  üìÑ Procesando p√°gina 1/1\n",
      "\n",
      "0: 640x480 1 customer_info, 3 invoice_details, 2 logos, 5 paragraphs, 3 tables, 66.6ms\n",
      "Speed: 2.2ms preprocess, 66.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "    ‚úÖ Se encontraron 14 detecciones\n",
      "      üíæ Guardado: pag1_det1_logo_conf0.83.jpg\n",
      "      üíæ Guardado: pag1_det2_customer_info_conf0.74.jpg\n",
      "      üíæ Guardado: pag1_det3_paragraph_conf0.69.jpg\n",
      "      üíæ Guardado: pag1_det4_invoice_detail_conf0.66.jpg\n",
      "      üíæ Guardado: pag1_det5_table_conf0.59.jpg\n",
      "      üíæ Guardado: pag1_det6_invoice_detail_conf0.54.jpg\n",
      "      üíæ Guardado: pag1_det7_paragraph_conf0.51.jpg\n",
      "      üíæ Guardado: pag1_det8_paragraph_conf0.46.jpg\n",
      "      üíæ Guardado: pag1_det9_table_conf0.46.jpg\n",
      "      üíæ Guardado: pag1_det10_paragraph_conf0.39.jpg\n",
      "      üíæ Guardado: pag1_det11_logo_conf0.36.jpg\n",
      "      üíæ Guardado: pag1_det12_table_conf0.33.jpg\n",
      "      üíæ Guardado: pag1_det13_paragraph_conf0.31.jpg\n",
      "      üíæ Guardado: pag1_det14_invoice_detail_conf0.25.jpg\n",
      "\n",
      "üîç Procesando: factura-5053_jr0yl0l5jp.pdf\n",
      "  üìÑ Procesando p√°gina 1/1\n",
      "\n",
      "0: 640x480 1 bank_info, 2 customer_infos, 1 logo, 1 paragraph, 1 table, 2 total_prices, 1 vendor_info, 66.7ms\n",
      "Speed: 1.5ms preprocess, 66.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "    ‚úÖ Se encontraron 9 detecciones\n",
      "      üíæ Guardado: pag1_det1_customer_info_conf0.80.jpg\n",
      "      üíæ Guardado: pag1_det2_logo_conf0.73.jpg\n",
      "      üíæ Guardado: pag1_det3_vendor_info_conf0.56.jpg\n",
      "      üíæ Guardado: pag1_det4_customer_info_conf0.52.jpg\n",
      "      üíæ Guardado: pag1_det5_bank_info_conf0.50.jpg\n",
      "      üíæ Guardado: pag1_det6_paragraph_conf0.45.jpg\n",
      "      üíæ Guardado: pag1_det7_table_conf0.37.jpg\n",
      "      üíæ Guardado: pag1_det8_total_price_conf0.33.jpg\n",
      "      üíæ Guardado: pag1_det9_total_price_conf0.29.jpg\n",
      "\n",
      "üîç Procesando: op-1632031_ufzh8cap6i.pdf\n",
      "  üìÑ Procesando p√°gina 1/2\n",
      "\n",
      "0: 640x480 1 invoice_detail, 2 logos, 3 paragraphs, 1 table, 4 total_prices, 1 vendor_info, 66.5ms\n",
      "Speed: 1.5ms preprocess, 66.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "    ‚úÖ Se encontraron 12 detecciones\n",
      "      üíæ Guardado: pag1_det1_vendor_info_conf0.60.jpg\n",
      "      üíæ Guardado: pag1_det2_total_price_conf0.58.jpg\n",
      "      üíæ Guardado: pag1_det3_total_price_conf0.56.jpg\n",
      "      üíæ Guardado: pag1_det4_total_price_conf0.54.jpg\n",
      "      üíæ Guardado: pag1_det5_table_conf0.51.jpg\n",
      "      üíæ Guardado: pag1_det6_paragraph_conf0.48.jpg\n",
      "      üíæ Guardado: pag1_det7_invoice_detail_conf0.48.jpg\n",
      "      üíæ Guardado: pag1_det8_total_price_conf0.45.jpg\n",
      "      üíæ Guardado: pag1_det9_logo_conf0.40.jpg\n",
      "      üíæ Guardado: pag1_det10_paragraph_conf0.38.jpg\n",
      "      üíæ Guardado: pag1_det11_logo_conf0.34.jpg\n",
      "      üíæ Guardado: pag1_det12_paragraph_conf0.27.jpg\n",
      "  üìÑ Procesando p√°gina 2/2\n",
      "\n",
      "0: 640x480 1 customer_info, 1 invoice_detail, 4 logos, 6 paragraphs, 1 total_price, 2 vendor_infos, 65.5ms\n",
      "Speed: 1.4ms preprocess, 65.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "    ‚úÖ Se encontraron 15 detecciones\n",
      "      üíæ Guardado: pag2_det1_vendor_info_conf0.76.jpg\n",
      "      üíæ Guardado: pag2_det2_paragraph_conf0.73.jpg\n",
      "      üíæ Guardado: pag2_det3_logo_conf0.69.jpg\n",
      "      üíæ Guardado: pag2_det4_vendor_info_conf0.60.jpg\n",
      "      üíæ Guardado: pag2_det5_logo_conf0.59.jpg\n",
      "      üíæ Guardado: pag2_det6_logo_conf0.51.jpg\n",
      "      üíæ Guardado: pag2_det7_paragraph_conf0.48.jpg\n",
      "      üíæ Guardado: pag2_det8_paragraph_conf0.44.jpg\n",
      "      üíæ Guardado: pag2_det9_total_price_conf0.37.jpg\n",
      "      üíæ Guardado: pag2_det10_invoice_detail_conf0.36.jpg\n",
      "      üíæ Guardado: pag2_det11_paragraph_conf0.33.jpg\n",
      "      üíæ Guardado: pag2_det12_customer_info_conf0.31.jpg\n",
      "      üíæ Guardado: pag2_det13_logo_conf0.30.jpg\n",
      "      üíæ Guardado: pag2_det14_paragraph_conf0.27.jpg\n",
      "      üíæ Guardado: pag2_det15_paragraph_conf0.27.jpg\n",
      "\n",
      "‚úÖ Proceso completado. Resultados en: resultados_clasificacion/\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "from pathlib import Path\n",
    "\n",
    "def clasificar_pdfs():\n",
    "    # Configuraci√≥n de rutas\n",
    "    modelo_path = \"best.pt\"\n",
    "    carpeta_pdfs = \"dataset_analisis\"\n",
    "    carpeta_salida = \"resultados_clasificacion\"\n",
    "    \n",
    "    # Cargar modelo\n",
    "    print(\"üîÑ Cargando modelo...\")\n",
    "    model = YOLO(modelo_path)\n",
    "    \n",
    "    # Crear carpeta de salida\n",
    "    os.makedirs(carpeta_salida, exist_ok=True)\n",
    "    \n",
    "    # Obtener todos los PDFs\n",
    "    pdfs = [f for f in os.listdir(carpeta_pdfs) if f.endswith('.pdf')]\n",
    "    \n",
    "    print(f\"üìÑ Se encontraron {len(pdfs)} archivos PDF\")\n",
    "    \n",
    "    for pdf_file in pdfs:\n",
    "        print(f\"\\nüîç Procesando: {pdf_file}\")\n",
    "        pdf_path = os.path.join(carpeta_pdfs, pdf_file)\n",
    "        \n",
    "        # Crear carpeta para este PDF\n",
    "        nombre_sin_ext = Path(pdf_file).stem\n",
    "        carpeta_pdf = os.path.join(carpeta_salida, nombre_sin_ext)\n",
    "        os.makedirs(carpeta_pdf, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # Convertir PDF a im√°genes (una por p√°gina)\n",
    "            imagenes = convert_from_path(pdf_path, dpi=300)\n",
    "            \n",
    "            for idx_pagina, imagen_pil in enumerate(imagenes):\n",
    "                print(f\"  üìÑ Procesando p√°gina {idx_pagina + 1}/{len(imagenes)}\")\n",
    "                \n",
    "                # Convertir PIL Image a formato OpenCV\n",
    "                imagen_cv = cv2.cvtColor(np.array(imagen_pil), cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                # Hacer predicci√≥n\n",
    "                results = model(imagen_cv)\n",
    "                \n",
    "                # Extraer detecciones\n",
    "                detecciones = results[0].boxes\n",
    "                \n",
    "                if len(detecciones) > 0:\n",
    "                    print(f\"    ‚úÖ Se encontraron {len(detecciones)} detecciones\")\n",
    "                    \n",
    "                    for idx_det, box in enumerate(detecciones):\n",
    "                        # Obtener coordenadas\n",
    "                        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                        confianza = float(box.conf[0])\n",
    "                        clase = int(box.cls[0])\n",
    "                        nombre_clase = model.names[clase]\n",
    "                        \n",
    "                        # Recortar regi√≥n detectada\n",
    "                        recorte = imagen_cv[y1:y2, x1:x2]\n",
    "                        \n",
    "                        # Guardar recorte\n",
    "                        nombre_archivo = f\"pag{idx_pagina + 1}_det{idx_det + 1}_{nombre_clase}_conf{confianza:.2f}.jpg\"\n",
    "                        ruta_guardado = os.path.join(carpeta_pdf, nombre_archivo)\n",
    "                        cv2.imwrite(ruta_guardado, recorte)\n",
    "                        \n",
    "                        print(f\"      üíæ Guardado: {nombre_archivo}\")\n",
    "                else:\n",
    "                    print(f\"    ‚ö†Ô∏è  No se encontraron detecciones en esta p√°gina\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error procesando {pdf_file}: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Proceso completado. Resultados en: {carpeta_salida}/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    clasificar_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "def extraer_texto_con_ocr():\n",
    "    # Configuraci√≥n de rutas\n",
    "    carpeta_imagenes = \"resultados_clasificacion\"\n",
    "    carpeta_salida = \"resultados_ocr\"\n",
    "    \n",
    "    # Crear carpeta de salida\n",
    "    os.makedirs(carpeta_salida, exist_ok=True)\n",
    "    \n",
    "    # Inicializar EasyOCR (espa√±ol e ingl√©s)\n",
    "    print(\"üîÑ Inicializando EasyOCR...\")\n",
    "    reader = easyocr.Reader(['es', 'en'], gpu=False)\n",
    "    \n",
    "    print(\"‚úÖ EasyOCR listo\\n\")\n",
    "    \n",
    "    # Recorrer todas las carpetas de PDFs\n",
    "    carpetas_pdfs = [d for d in os.listdir(carpeta_imagenes) \n",
    "                     if os.path.isdir(os.path.join(carpeta_imagenes, d))]\n",
    "    \n",
    "    print(f\"üìÅ Se encontraron {len(carpetas_pdfs)} carpetas de PDFs\\n\")\n",
    "    \n",
    "    for carpeta_pdf in carpetas_pdfs:\n",
    "        print(f\"üîç Procesando carpeta: {carpeta_pdf}\")\n",
    "        \n",
    "        ruta_carpeta_pdf = os.path.join(carpeta_imagenes, carpeta_pdf)\n",
    "        \n",
    "        # Crear carpeta de salida para este PDF\n",
    "        carpeta_salida_pdf = os.path.join(carpeta_salida, carpeta_pdf)\n",
    "        os.makedirs(carpeta_salida_pdf, exist_ok=True)\n",
    "        \n",
    "        # Obtener todas las im√°genes\n",
    "        imagenes = [f for f in os.listdir(ruta_carpeta_pdf) \n",
    "                   if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        \n",
    "        print(f\"  üì∑ Total de im√°genes: {len(imagenes)}\")\n",
    "        \n",
    "        # Resultados consolidados\n",
    "        resultados_pdf = []\n",
    "        \n",
    "        for idx, imagen_file in enumerate(sorted(imagenes), 1):\n",
    "            ruta_imagen = os.path.join(ruta_carpeta_pdf, imagen_file)\n",
    "            \n",
    "            print(f\"    üîé [{idx}/{len(imagenes)}] Procesando: {imagen_file}\")\n",
    "            \n",
    "            try:\n",
    "                # Leer texto con EasyOCR\n",
    "                resultado = reader.readtext(ruta_imagen)\n",
    "                \n",
    "                # Extraer solo el texto\n",
    "                textos = [deteccion[1] for deteccion in resultado]\n",
    "                texto_completo = \" \".join(textos)\n",
    "                \n",
    "                # Calcular precisi√≥n promedio\n",
    "                if resultado:\n",
    "                    precision_promedio = sum(det[2] for det in resultado) / len(resultado)\n",
    "                else:\n",
    "                    precision_promedio = 0.0\n",
    "                \n",
    "                # Informaci√≥n detallada\n",
    "                info_detallada = {\n",
    "                    \"archivo\": imagen_file,\n",
    "                    \"texto_completo\": texto_completo,\n",
    "                    \"precision_promedio\": float(precision_promedio),\n",
    "                    \"num_detecciones\": len(resultado),\n",
    "                    \"detecciones\": [\n",
    "                        {\n",
    "                            \"texto\": det[1],\n",
    "                            \"confianza\": float(det[2]),\n",
    "                            \"coordenadas\": [int(coord) for punto in det[0] for coord in punto]\n",
    "                        }\n",
    "                        for det in resultado\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                resultados_pdf.append(info_detallada)\n",
    "                \n",
    "                # Guardar texto en archivo individual CON PRECISI√ìN PROMEDIO\n",
    "                nombre_txt = Path(imagen_file).stem + \".txt\"\n",
    "                ruta_txt = os.path.join(carpeta_salida_pdf, nombre_txt)\n",
    "                with open(ruta_txt, 'w', encoding='utf-8') as f:\n",
    "                    if texto_completo:\n",
    "                        f.write(f\"{texto_completo} (precisi√≥n OCR: {precision_promedio:.2%})\")\n",
    "                    else:\n",
    "                        f.write(\"No se detect√≥ texto\")\n",
    "                \n",
    "                if texto_completo:\n",
    "                    print(f\"      ‚úÖ Texto extra√≠do: {len(textos)} l√≠neas\")\n",
    "                    print(f\"      üìä Precisi√≥n promedio: {precision_promedio:.2%}\")\n",
    "                    print(f\"      üìù Preview: {texto_completo[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"      ‚ö†Ô∏è  No se detect√≥ texto\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ùå Error: {e}\")\n",
    "                resultados_pdf.append({\n",
    "                    \"archivo\": imagen_file,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        # Guardar resultados consolidados en JSON\n",
    "        archivo_json = os.path.join(carpeta_salida_pdf, \"resultados_completos.json\")\n",
    "        with open(archivo_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(resultados_pdf, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"  üíæ Resultados guardados en: {carpeta_salida_pdf}/\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Proceso completado. Resultados en: {carpeta_salida}/\")\n",
    "    \n",
    "    # Crear resumen general\n",
    "    crear_resumen_general(carpeta_salida)\n",
    "\n",
    "def crear_resumen_general(carpeta_salida):\n",
    "    \"\"\"Crea un resumen de todos los textos extra√≠dos\"\"\"\n",
    "    print(\"\\nüìä Generando resumen general...\")\n",
    "    \n",
    "    resumen = {}\n",
    "    \n",
    "    carpetas = [d for d in os.listdir(carpeta_salida) \n",
    "                if os.path.isdir(os.path.join(carpeta_salida, d))]\n",
    "    \n",
    "    for carpeta in carpetas:\n",
    "        ruta_json = os.path.join(carpeta_salida, carpeta, \"resultados_completos.json\")\n",
    "        \n",
    "        if os.path.exists(ruta_json):\n",
    "            with open(ruta_json, 'r', encoding='utf-8') as f:\n",
    "                datos = json.load(f)\n",
    "                \n",
    "            # Calcular precisi√≥n promedio general\n",
    "            precisiones = [d.get(\"precision_promedio\", 0) for d in datos if \"precision_promedio\" in d]\n",
    "            precision_general = sum(precisiones) / len(precisiones) if precisiones else 0\n",
    "                \n",
    "            resumen[carpeta] = {\n",
    "                \"total_imagenes\": len(datos),\n",
    "                \"imagenes_con_texto\": sum(1 for d in datos if d.get(\"texto_completo\")),\n",
    "                \"total_detecciones\": sum(d.get(\"num_detecciones\", 0) for d in datos),\n",
    "                \"precision_promedio\": f\"{precision_general:.2%}\"\n",
    "            }\n",
    "    \n",
    "    # Guardar resumen\n",
    "    archivo_resumen = os.path.join(carpeta_salida, \"resumen_general.json\")\n",
    "    with open(archivo_resumen, 'w', encoding='utf-8') as f:\n",
    "        json.dump(resumen, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Resumen guardado en: {archivo_resumen}\")\n",
    "    \n",
    "    # Mostrar resumen en consola\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RESUMEN GENERAL\")\n",
    "    print(\"=\"*50)\n",
    "    for pdf, stats in resumen.items():\n",
    "        print(f\"\\nüìÑ {pdf}:\")\n",
    "        print(f\"   Total im√°genes: {stats['total_imagenes']}\")\n",
    "        print(f\"   Con texto: {stats['imagenes_con_texto']}\")\n",
    "        print(f\"   Detecciones: {stats['total_detecciones']}\")\n",
    "        print(f\"   Precisi√≥n promedio: {stats['precision_promedio']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extraer_texto_con_ocr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a7a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Cargando modelos...\n",
      "‚úÖ Modelos cargados\n",
      "\n",
      "üöÄ Servidor FastAPI iniciado en http://0.0.0.0:8090\n",
      "üìñ Documentaci√≥n disponible en http://0.0.0.0:8090/docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [96659]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8090 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 2 customer_infos, 2 invoice_details, 2 logos, 5 paragraphs, 2 tables, 77.2ms\n",
      "Speed: 2.0ms preprocess, 77.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52066 - \"POST /procesar-pdf/ HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "# Ahora ejecuta tu c√≥digo FastAPI\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from fastapi.responses import JSONResponse\n",
    "from ultralytics import YOLO\n",
    "import easyocr\n",
    "import cv2\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import os\n",
    "from typing import Dict\n",
    "import uvicorn\n",
    "\n",
    "# Inicializar FastAPI\n",
    "app = FastAPI(title=\"PDF OCR API\", version=\"1.0.0\")\n",
    "\n",
    "# Cargar modelo YOLO y EasyOCR al inicio\n",
    "print(\"üîÑ Cargando modelos...\")\n",
    "modelo_yolo = YOLO(\"best.pt\")\n",
    "reader_ocr = easyocr.Reader(['es', 'en'], gpu=False)\n",
    "print(\"‚úÖ Modelos cargados\\n\")\n",
    "\n",
    "def procesar_pdf(pdf_path: str) -> Dict[str, list]:\n",
    "    \"\"\"\n",
    "    Procesa un PDF: extrae im√°genes, clasifica con YOLO, aplica OCR\n",
    "    Retorna diccionario con resultados por clase (acumulando m√∫ltiples detecciones)\n",
    "    \"\"\"\n",
    "    resultados = {}\n",
    "    \n",
    "    try:\n",
    "        # Abrir PDF con PyMuPDF\n",
    "        doc = fitz.open(pdf_path)\n",
    "        \n",
    "        for idx_pagina in range(len(doc)):\n",
    "            # Obtener la p√°gina\n",
    "            pagina = doc[idx_pagina]\n",
    "            \n",
    "            # Convertir p√°gina a imagen\n",
    "            mat = fitz.Matrix(2, 2)\n",
    "            pix = pagina.get_pixmap(matrix=mat)\n",
    "            \n",
    "            # Convertir a numpy array\n",
    "            img_data = np.frombuffer(pix.samples, dtype=np.uint8).reshape(\n",
    "                pix.height, pix.width, pix.n\n",
    "            )\n",
    "            \n",
    "            # Convertir a BGR (OpenCV)\n",
    "            if pix.n == 4:  # RGBA\n",
    "                imagen_cv = cv2.cvtColor(img_data, cv2.COLOR_RGBA2BGR)\n",
    "            else:  # RGB\n",
    "                imagen_cv = cv2.cvtColor(img_data, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Hacer predicci√≥n con YOLO\n",
    "            results = modelo_yolo(imagen_cv)\n",
    "            detecciones = results[0].boxes\n",
    "            \n",
    "            # Procesar cada detecci√≥n\n",
    "            for box in detecciones:\n",
    "                # Obtener coordenadas y clase\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                confianza_yolo = float(box.conf[0])\n",
    "                clase = int(box.cls[0])\n",
    "                nombre_clase = modelo_yolo.names[clase]\n",
    "                \n",
    "                # Validar coordenadas\n",
    "                if x2 > x1 and y2 > y1:\n",
    "                    # Recortar regi√≥n detectada\n",
    "                    recorte = imagen_cv[y1:y2, x1:x2]\n",
    "                    \n",
    "                    # Aplicar OCR al recorte\n",
    "                    resultado_ocr = reader_ocr.readtext(recorte)\n",
    "                    \n",
    "                    if resultado_ocr:\n",
    "                        # Extraer texto y calcular precisi√≥n promedio\n",
    "                        textos = [det[1] for det in resultado_ocr]\n",
    "                        texto_completo = \" \".join(textos)\n",
    "                        precision_promedio = sum(det[2] for det in resultado_ocr) / len(resultado_ocr)\n",
    "                        \n",
    "                        # NUEVA ESTRUCTURA: Diccionario con texto y precisi√≥n\n",
    "                        if texto_completo.strip():\n",
    "                            deteccion = {\n",
    "                                \"text\": texto_completo,\n",
    "                                \"precision\": f\"{precision_promedio:.2%}\",\n",
    "                                # \"precision_raw\": round(precision_promedio, 4),\n",
    "                                # \"pagina\": idx_pagina + 1,\n",
    "                                # \"confianza_yolo\": f\"{confianza_yolo:.2%}\"\n",
    "                            }\n",
    "                            \n",
    "                            # Inicializar lista si no existe\n",
    "                            if nombre_clase not in resultados:\n",
    "                                resultados[nombre_clase] = []\n",
    "                            \n",
    "                            # Agregar a la lista\n",
    "                            resultados[nombre_clase].append(deteccion)\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error procesando PDF: {str(e)}\")\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "@app.post(\"/procesar-pdf/\")\n",
    "async def procesar_pdf_endpoint(file: UploadFile = File(...)):\n",
    "    \"\"\"\n",
    "    Endpoint para procesar un archivo PDF\n",
    "    \n",
    "    Par√°metros:\n",
    "    - file: Archivo PDF a procesar\n",
    "    \n",
    "    Retorna:\n",
    "    - JSON con resultados del OCR organizados por clase\n",
    "    \"\"\"\n",
    "    # Validar que sea un PDF\n",
    "    if not file.filename.endswith('.pdf'):\n",
    "        raise HTTPException(status_code=400, detail=\"El archivo debe ser un PDF\")\n",
    "    \n",
    "    # Crear archivo temporal\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:\n",
    "        # Guardar archivo subido\n",
    "        contenido = await file.read()\n",
    "        temp_file.write(contenido)\n",
    "        temp_path = temp_file.name\n",
    "    \n",
    "    try:\n",
    "        # Procesar PDF\n",
    "        resultados = procesar_pdf(temp_path)\n",
    "        \n",
    "        if not resultados:\n",
    "            return JSONResponse(\n",
    "                content={\"mensaje\": \"No se detect√≥ texto en el PDF\"},\n",
    "                status_code=200\n",
    "            )\n",
    "        \n",
    "        return JSONResponse(content=resultados, status_code=200)\n",
    "        \n",
    "    finally:\n",
    "        # Limpiar archivo temporal\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Endpoint de bienvenida\"\"\"\n",
    "    return {\n",
    "        \"mensaje\": \"API de procesamiento de PDFs con YOLO y OCR\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"endpoints\": {\n",
    "            \"/procesar-pdf/\": \"POST - Sube un PDF para extraer texto\",\n",
    "            \"/docs\": \"GET - Documentaci√≥n interactiva\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Verificar estado de la API\"\"\"\n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"modelo_yolo\": \"cargado\",\n",
    "        \"ocr\": \"cargado\"\n",
    "    }\n",
    "\n",
    "# Ejecutar con nest_asyncio en Jupyter\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    from threading import Thread\n",
    "    \n",
    "    def run_server():\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=8090)\n",
    "    \n",
    "    # Ejecutar el servidor en un hilo separado para Jupyter\n",
    "    server_thread = Thread(target=run_server, daemon=True)\n",
    "    server_thread.start()\n",
    "    \n",
    "    print(\"üöÄ Servidor FastAPI iniciado en http://0.0.0.0:8090\")\n",
    "    print(\"üìñ Documentaci√≥n disponible en http://0.0.0.0:8090/docs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
